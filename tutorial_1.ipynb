{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of WSI processing\n",
    "\n",
    " This notebook will guide you through the basics of WSI processing.\n",
    " Because of the size of the data/models this course would like to introduce, note that\n",
    " most of the heavy computations have been done in advance: training, embeddings; LLM are run in the cloud.\n",
    " This TP will focus on using the models, understanding their capabilities and limitations, rather than training them.\n",
    "\n",
    " At the end of this notebook, you should be familiar with:\n",
    " - Handling WSI data and understanding their pyramidal structure.\n",
    " - Extract tiles from a WSI, at any resolution, only in the tissue regions.\n",
    " - Get an intuition about tiles embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_IMPORTANT_** First thing: If you have not done it yet, download this https://drive.google.com/file/d/1VN57GS0d-fVQkBlc63UC7jVXxnw4Dw_W/view?usp=sharing and untar it in this folder.\n",
    "You should now have an `./assets` subfolder.\n",
    "\n",
    "**_IMPORTANT BIS_** Second, please download the following tar and untar it in your `./assets` folder! \n",
    "These are useful data that I forgot to put in the initial tar\n",
    "\n",
    "LINK : https://drive.google.com/file/d/1pX_Ai2rXVnPhk4vdk8cEqoQWeY57osn7/view?usp=sharing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ‘€â€‹ How to use these notebooks ?\n",
    "> Go through the notebooks cells by cells, they are not independant and you will need previous cells to run the later ones.\n",
    "> Many questions are scattered throughout the notebooks. Questions in the first notebook are interesting to get used to WSI manipulation, \n",
    "> questions in the second may be a bit broader and open: you are free to solve these questions yourself (what I would advise) or to just copy the answers present in the `answers.py` file.\n",
    "> Open questions will be adressed orally. Alternatively, you may find the full corrected notebook in the first commit of this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import json\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import openslide\n",
    "import torch\n",
    "import einops\n",
    "from PIL import Image\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.table import Table\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.morphology import closing, opening, footprint_rectangle\n",
    "from tqdm import tqdm\n",
    "os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsi_path = \"./assets/TCGA-Z7-A8R6-01Z-00-DX1.CE4ED818-D762-4324-9DEA-2ACB38B9B0B9.svs\"\n",
    "wsi_name = Path(wsi_path).stem\n",
    "wsi = openslide.open_slide(wsi_path)\n",
    "wsi.get_thumbnail((512, 512)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Pyramidal Structure of WSIs\n",
    "\n",
    "Whole Slide Images are stored in a specialized pyramidal format that optimizes their accessibility and viewing. This structure consists of multiple resolution levels, where each level represents a progressively downsampled version of the original image. Each level is stored on disk: it increases the global weight of the object but make it easier to manipulate it.\n",
    "\n",
    " Level 0 is the original resolution - very often corresponding to a 40x magnification (around 0.25 microns per pixel).\n",
    " The downsampling factor between each level is usually 2, but may vary depending on the scanner used - \n",
    " many of the slides of the TCGA, scanned on an Aperio scanner, have a downsampling factor of 4.\n",
    "\n",
    " Because maximum resolution and downsampling factors may depend on the scanner, there is not a one-one correspondance between a slide *level* and a *magnification* - finding this correspondance is left to the user (or software) given the metadata.\n",
    "\n",
    " You can access all these information using the OpenSlide object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()\n",
    "\n",
    "# Create a table for WSI information\n",
    "table = Table(title=\"WSI Information\")\n",
    "table.add_column(\"Property\", style=\"cyan\")\n",
    "table.add_column(\"Value\", style=\"green\")\n",
    "table.add_row(\"Level count\", str(wsi.level_count))\n",
    "table.add_row(\"Level dimensions\", str(wsi.level_dimensions))\n",
    "table.add_row(\"Level downsamples\", str(wsi.level_downsamples))\n",
    "console.print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dimensions = wsi.dimensions\n",
    "console.print(Panel(f\"Slide dimensions: {dimensions}\", title=\"Slide Size\"))\n",
    "\n",
    "properties_table = Table(title=\"Slide Properties\")\n",
    "properties_table.add_column(\"Property\", style=\"cyan\")\n",
    "properties_table.add_column(\"Value\", style=\"green\")\n",
    "\n",
    "for key, value in wsi.properties.items():\n",
    "    properties_table.add_row(key, str(value))\n",
    "\n",
    "console.print(properties_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize this a bit !\n",
    "# let's try to pick a region at the center of the slide (to have sone tissue in it).\n",
    "\n",
    "center_x = dimensions[0] // 2\n",
    "center_y = dimensions[1] // 2\n",
    "region_size = (256, 256)\n",
    "\n",
    "region_high = wsi.read_region(\n",
    "    (center_x - region_size[0]//2, center_y - region_size[1]//2),\n",
    "    0,\n",
    "    region_size\n",
    ")\n",
    "\n",
    "region_mid = wsi.read_region(\n",
    "    (center_x, center_y),\n",
    "    1,\n",
    "    region_size\n",
    ")\n",
    "\n",
    "region_low = wsi.read_region(\n",
    "    (center_x, center_y),\n",
    "    2,\n",
    "    region_size\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(region_high)\n",
    "axes[0].set_title('Level 0 (High Resolution)')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(region_mid)\n",
    "axes[1].set_title('Level 1 (Medium Resolution)')\n",
    "axes[1].axis('off')\n",
    "axes[2].imshow(region_low)\n",
    "axes[2].set_title('Level 2 (Low Resolution)')\n",
    "axes[2].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question 1.1**: Resolution Analysis\n",
    "> Using the OpenSlide object properties, determine:\n",
    "> 1. The resolution of each pyramid level\n",
    "> 2. Express the results in terms of:\n",
    ">    - Magnification (e.g., 40x, 20x, etc.)\n",
    ">    - Microns per pixel (Î¼m/pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "def get_magnification(level):\n",
    "   pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question 1.2**: Image Scale Analysis\n",
    "> Calculate the size of the highest resolution level (Level 0):\n",
    "> - Determine the total number of pixels\n",
    "> - Consider why working with the full resolution directly might be challenging\n",
    "> - Think about memory implications when processing such large images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Tiling Strategy for WSI Analysis\n",
    "\n",
    "### Why Tiling?\n",
    "As demonstrated by our previous calculations, working with the entire WSI at full resolution is computationally impractical due to its massive size. Instead, we'll break down the image into manageable tiles for processing.\n",
    "\n",
    "### Approach\n",
    "- **Grid Generation**: Create a systematic grid of tiles across the WSI\n",
    "- **Tissue Detection**: Filter tiles to keep only those containing relevant tissue\n",
    "- **Encoding**: encode the tile information using pre-trained models\n",
    "\n",
    "### Implementation Note\n",
    "We'll use OpenSlide for this implementation, though modern alternatives like CuCIM exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for tiling\n",
    "tile_size = 256  # Final tile size for deep learning models\n",
    "magnification_tile = 10  # Target magnification (10x)\n",
    "mask_tolerance = 0.9  # Tissue detection threshold (90% tissue required)\n",
    "\n",
    "# Calculate which pyramid level to use for the target magnification\n",
    "mag_level0 = 40  # get that from the slide object. \n",
    "ds_per_level = {'.svs': 4, '.ndpi': 2, '.tiff': 2, '.tif': 2}  # Downsampling factors - it can indeed depend on the scanner. To simplify, because tcga are svs and most slides are taken using a 4x downsampling Aperio scanner, I shortcut svs: x4.\n",
    "ext = Path(wsi_path).suffix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to sample a tile at a given magnification (for insance, we want to extract tiles at 10x) \n",
    "> **_Question 1.3_** Given the magnification at level 0 and the downsampling factor, compute the level for extracting the slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tissue Detection Strategy\n",
    "\n",
    "Our goal is to extract tiles that contain meaningful tissue content while excluding background areas. We'll implement this using a two-step approach:\n",
    "\n",
    "1. **Tissue Mask Creation**\n",
    "   - Use Otsu thresholding to separate tissue from background\n",
    "   - Apply morphological operations to clean up the mask\n",
    "\n",
    "2. **Tile Filtering**\n",
    "   - Calculate tissue content percentage for each tile\n",
    "   - Keep tiles exceeding a specified tissue threshold\n",
    "\n",
    "**Implementation Note**: This is a basic, handcrafted approach to tissue detection. In practice, you might want to consider:\n",
    " - More sophisticated tissue detection algorithms\n",
    " - Deep learning-based tissue classifiers\n",
    " - Additional filtering for artifacts and scanning errors\n",
    " - Domain-specific adaptations for different tissue types\n",
    "\n",
    " An simple alternative to this masking strategy is to design a tile classifier to exclude background tiles.\n",
    " It can be as simple as a thresold on the spatial average of the pixel values, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_auto_mask(thumbnail):\n",
    "    \"\"\"Create a binary mask from thumbnail using Otsu algorithm and morphological operations.\"\"\"\n",
    "    im = np.array(thumbnail)[:, :, :3]\n",
    "    im_gray = rgb2gray(im)\n",
    "    size = im_gray.shape\n",
    "    im_gray = im_gray.flatten()\n",
    "    \n",
    "    pixels_int = im_gray[np.logical_and(im_gray > 0.02, im_gray < 0.98)]\n",
    "    \n",
    "    if len(pixels_int) == 0:\n",
    "        return np.zeros(size, dtype=bool)\n",
    "    \n",
    "    # Apply Otsu thresholding\n",
    "    threshold = threshold_otsu(pixels_int)\n",
    "    mask = (im_gray < threshold).reshape(size)\n",
    "    \n",
    "    # Apply morphological operations to clean up the mask\n",
    "    mask = opening(closing(mask, footprint_rectangle((2,2))), footprint_rectangle((2,2)))\n",
    "    return mask\n",
    "\n",
    "# Get thumbnail and create tissue mask\n",
    "thumbnail = wsi.get_thumbnail(wsi.level_dimensions[-1])  # Use lowest resolution level\n",
    "tissue_mask = make_auto_mask(thumbnail)\n",
    "\n",
    "# Visualize tissue detection\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axes[0].imshow(thumbnail)\n",
    "axes[0].set_title('Original Thumbnail')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(tissue_mask)\n",
    "axes[1].set_title(f'Tissue Mask ({tissue_mask.sum()/tissue_mask.size*100:.1f}% tissue)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea now is to create a grid (i.e. a set of coordinates that regularly span the image), and check, for each tile of the grid, how much falls into the mask.\n",
    "\n",
    "> **_Question 1.4_** Create this grid of tiles! They can be overlapping or not - You could add this as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "def grid_coords(slide, point_start, point_end, patch_size):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is now several ways to check if the tile falls into the mask.\n",
    "Either downsample the tile to the mask level or the reverse. \n",
    "> **_Question 1.5_** Implement one of these solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "def check_coordinates(row, col, patch_size, mask, mask_downsample, mask_tolerance=0.9):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the complete grid -  we generate it at resolution 0\n",
    "image_height, image_width = wsi.dimensions[1], wsi.dimensions[0]\n",
    "mask_downsample = int(wsi.level_downsamples[-1])  # Downsample factor for mask level\n",
    "size_at_0 = tile_size * (ds_per_level[ext] ** level_tile)  # Tile size at level 0 coordinates\n",
    "\n",
    "# Create uniform grid\n",
    "all_coordinates = grid_coords(wsi, (0, 0), (image_height, image_width), (size_at_0, size_at_0))\n",
    "\n",
    "console.print(f\"Generated {len(all_coordinates):,} potential tile locations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Filter grid based on tissue content\n",
    "valid_tiles = []\n",
    "for row, col in all_coordinates:\n",
    "    if check_coordinates(row, col, (size_at_0, size_at_0), tissue_mask, mask_downsample, mask_tolerance):\n",
    "        valid_tiles.append((row, col, size_at_0, size_at_0))\n",
    "\n",
    "console.print(Panel(f\"Grid Generation Results:\\nâ€¢ Total potential tiles: {len(all_coordinates):,}\\nâ€¢ Valid tiles (â‰¥{mask_tolerance*100:.0f}% tissue): {len(valid_tiles):,}\\nâ€¢ Tissue proportion: {len(valid_tiles)/len(all_coordinates)*100:.1f}%\", title=\"Tiling Results\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize where the tiles are localized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "\n",
    "# Show tissue mask with valid tile locations\n",
    "ax.imshow(thumbnail)\n",
    "ax.set_title('WSI Thumbnail + Tiling Grid (sparse)')\n",
    "\n",
    "# we will show only a subset of the tiles - for visibiity\n",
    "sparse_tiles = valid_tiles[::max(1, len(valid_tiles)//200)]\n",
    "for row, col, height, width in sparse_tiles:\n",
    "    mask_row = row // mask_downsample\n",
    "    mask_col = col // mask_downsample\n",
    "    mask_height = height // mask_downsample\n",
    "    mask_width = width // mask_downsample\n",
    "    \n",
    "    rect = patches.Rectangle((mask_col, mask_row), mask_width, mask_height,\n",
    "                           linewidth=1.5, edgecolor='lime', facecolor='none', alpha=0.9)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... And directly some of the tiles themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tile(wsi, row, col, level, tile_size):\n",
    "    \"\"\"Extract a single tile from the WSI.\"\"\"\n",
    "    return wsi.read_region(\n",
    "        location=(col, row), \n",
    "        level=level, \n",
    "        size=(tile_size, tile_size)\n",
    "    ).convert('RGB')\n",
    "\n",
    "n_sample_tiles = min(9, len(valid_tiles))\n",
    "sample_indices = np.random.choice(len(valid_tiles), n_sample_tiles, replace=False)\n",
    "sample_tiles = [valid_tiles[i] for i in sample_indices]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "fig.suptitle(f'Sample Extracted Tiles (Level {level_tile}, {magnification_tile}x magnification)', fontsize=14)\n",
    "\n",
    "for i, (row, col, height, width) in enumerate(sample_tiles):\n",
    "    ax = axes[i//3, i%3]\n",
    "    tile = extract_tile(wsi, row, col, level_tile, tile_size)\n",
    "    \n",
    "    ax.imshow(tile)\n",
    "    ax.set_title(f'Tile {i+1}\\n({row//1000}k, {col//1000}k)', fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "for i in range(n_sample_tiles, 9):\n",
    "    axes[i//3, i%3].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "console.print(Panel(f\"Successfully generated tiling grid!\\nâ€¢ Ready for tile extraction and processing\\nâ€¢ Grid contains {len(valid_tiles):,} valid tile locations\\nâ€¢ Each tile: {tile_size}Ã—{tile_size} pixels at {magnification_tile}Ã— magnification\", title=\"Tiling Complete\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile Embeddings with Pathology Foundation Models\n",
    "\n",
    "The second step is to extract features from pathology tiles, to then be aggregated at the slide level.\n",
    "\n",
    "### Evolution of WSI Feature Extraction\n",
    "The field of digital pathology has seen significant advancement in how we process and analyze WSI tiles.\n",
    "Before 2020 we used to use ImageNet-pretrained encoders.\n",
    "This strategy worked (surprisingly) well but was limited by the gap between natural images and histology.\n",
    "\n",
    "2020 has seen the rise of SSL (summer 2020 is the summer of SimCLR, MoCo, SimSiam etc...), which found in pathology a perfect application:\n",
    "we have a quasi unlimited amount of tiles but no labels!\n",
    "This strategy leverages self-supervised learning, trained on millions of pathology images.\n",
    "\n",
    "### Available Foundation Models\n",
    "Several state-of-the-art models are readily available on HuggingFace:\n",
    "\n",
    "| Model | Description | Link |\n",
    "|-------|-------------|------|\n",
    "| H-Optimus-0/1 | models trained on one of the biggest WSI dataset | [HuggingFace](https://huggingface.co/bioptimus/) |\n",
    "| UNI2 | on par with bioptimus models | [HuggingFace](https://huggingface.co/MahmoodLab/UNI2-h) |\n",
    "| Gigapath | They implement a rather basic tile-encoder (that works well) but mostly a slide encoder | [HuggingFace](https://huggingface.co/prov-gigapath/prov-gigapath) |\n",
    "\n",
    "### Current Landscape\n",
    "- New models are released frequently\n",
    "- Development requires significant computational resources i.e. they are led by major research labs and corporations\n",
    "- **Best Practice**: Use these pre-trained models out-of-the-box\n",
    "  - Proven strong performance on various downstream tasks\n",
    "  - Efficient transfer learning capabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding here is done using a pathology CLIP model, which will be described in greater details in the second notebook.\n",
    "# This is a particular type of image foundation model, trained with the supervision of text.\n",
    "def embed_batch(wsi, coordinates, model, preprocess):\n",
    "    tiles = [extract_tile(wsi, row, col, level_tile, tile_size) for row, col, _, _ in coordinates]\n",
    "    tiles = [preprocess(tile) for tile in tiles]\n",
    "    tiles = torch.stack(tiles)\n",
    "    tiles = model.encode_image(tiles)\n",
    "    return tiles\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16', pretrained='assets/pathgenclip.pt')\n",
    "batch_size = 20\n",
    "valid_tiles_batched = [valid_tiles[i:i+batch_size] for i in range(0, len(valid_tiles), batch_size)]\n",
    "\n",
    "embeddings = []\n",
    "for tile in tqdm(valid_tiles_batched):\n",
    "    embeddings.append(embed_batch(wsi, tile, model, preprocess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If you can't run that on your machine (this might take some time!), you could try:\n",
    " - enabling GPU acceleration if you got any (it would work on mac mps accelerator)\n",
    " - embed a **random** subset of the tiles\n",
    " - just use the pre-computed embeddings available here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings is here a matrix of size (n_tiles, 768)\n",
    "# Coordinates is a list of tuples (row, col, height, width) of size (n_tiles, 4)\n",
    "embeddings = np.load(\"./assets/embeddings_tiles.npy\")\n",
    "with open('./assets/coordinates_tiles.json', 'r') as f:\n",
    "    coordinates = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Question 1.6: Analyzing Embedding Space Structure\n",
    "\n",
    "> The latent space of tile embeddings (the high-dimensional manifold where embeddings reside) should be organized according to semantic similarity between images - if the pre-training was successful.\n",
    "> \n",
    "> **Task**: Let's verify this hypothesis through clustering analysis:\n",
    "> 1. Use K-means to cluster the embeddings\n",
    "> 2. Visualize the clusters using t-SNE dimensionality reduction\n",
    "> 3. Extract and examine example tiles from each cluster\n",
    "> 4. Analyze whether tiles within clusters share meaningful visual/semantic properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next challenge in computational pathology is to effectively aggregate these tile-level features to make a slide-level prediction. As pathologists know, critical diagnostic information often arises from the broader context, such as tumoral heterogeneity, the morphology of the invasive front, and the spatial arrangement of different tissue types.\n",
    "\n",
    "Most modern approaches to WSI analysis are inspired by Multiple Instance Learning (MIL). This framework treats each WSI as a \"bag\" of tile instances, where the overall slide label is determined by the properties of the tiles within it. A key advantage of MIL is its permutation invarianceâ€”the order of tiles in the bag doesn't matter.\n",
    "\n",
    "The seminal paper in this area, Attention-based MIL (abMIL), remains a powerful and relevant baseline today. The core idea is that not all tiles in the bag are equally important for the final diagnosis. For instance, tiles containing healthy fibrotic tissue might be irrelevant to a tumor classification task.\n",
    "\n",
    "AbMIL introduces an attention mechanism that learns to assign an importance score to each tile. A small neural network (often a simple two-layer MLP) computes these attention scores from the tile embeddings. The final slide representation is then created by taking a weighted average of all tile embeddings, guided by their learned attention. This allows the model to focus on the most informative regions of the slide.\n",
    "\n",
    "#### Further Reading & Advanced Models\n",
    "The field has evolved rapidly, with many powerful architectures building upon this foundation. Here are a few key papers for further reading:\n",
    "\n",
    "##### Seminal MIL Papers:\n",
    "**abMIL**: Ilse, M., Tomczak, J., & Welling, M. (2018). [ Attention-based Deep Multiple Instance Learning ](https://arxiv.org/abs/1802.04712).\n",
    "\n",
    "**dsMIL**: Li, B., Li, Y., & Eliceiri, K. W. (2021). [ Dual-stream Multiple Instance Learning Network for Whole Slide Image Classification ](https://arxiv.org/abs/2011.08939).\n",
    "\n",
    "**CLAM**: Lu, M. Y., Williamson, D. F., Chen, T. Y., Chen, R. J., Barbieri, M., & Mahmood, F. (2021). [ Data-Efficient and Weakly Supervised Computational Pathology on Whole-Slide Images ](https://www.nature.com/articles/s41551-020-00682-w).\n",
    "\n",
    "##### Slide-Level Foundation Models:\n",
    "More recently, work trained slide-level representations with self-supervision, giving rise to slide-foundation level, that can be used as-is for many downstream tasks.\n",
    "\n",
    "**Giga-SSL**: Lazard, T., Lerousseau, M., DecenciÃ¨re, E., & Walter, T. (2023). [ Giga-SSL: Self-Supervised Learning for Gigapixel Images ](https://arxiv.org/abs/2212.03273).\n",
    "\n",
    "**GigaPath**: Rao, A., et al. (2024). [ GigaPath: A Million-Slide Foundation Model for Computational Pathology ](https://www.nature.com/articles/s41586-024-07441-w).\n",
    "\n",
    "**PANTHER**: Singhal, K., et al. (2024). [ Pan-Cancer Histology-Genomic Integration at Scale ](https://arxiv.org/abs/2405.11643).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
