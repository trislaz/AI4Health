{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import os\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.text import Text\n",
    "from rich.table import Table\n",
    "import numpy as np\n",
    "import einops\n",
    "from glob import glob\n",
    "from huggingface_hub import InferenceClient\n",
    "import openai\n",
    "import base64\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "import math\n",
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "from openai import OpenAI\n",
    "\n",
    "console = Console()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# PathGen Tutorial: AI for Pathology\n",
    "\n",
    "This tutorial demonstrates the use of **PathGen-CLIP**, a specialized CLIP model for histopathology images, and explores the \"agentic\" pipeline for creating high-quality image-caption datasets in the pathology domain.\n",
    "\n",
    "## Background: CLIP Models and Vision-Language Alignment\n",
    "\n",
    "**CLIP (Contrastive Language-Image Pre-training)** models and their derivatives are Vision-Language Models (VLMs) that align vision and text modalities. Introduced by the [seminal work of Radford et al.](https://arxiv.org/abs/2103.00020), these models are trained using:\n",
    "\n",
    "- **Dataset**: Paired images and captions\n",
    "- **Objective**: Contrastive learning that brings closer the representations of positive pairs (image and its caption) while pushing apart negative pairs (image and captions of other images in the batch)\n",
    "\n",
    "This approach mirrors unimodal self-supervised contrastive learning but operates across modalities.\n",
    "\n",
    "## Applications in Visual Large Language Models\n",
    "\n",
    "CLIP models serve as the foundation for **VLLMs** (Visual Large Language Models) - systems that can process images and respond with text (like GPT-4o). Many  VLLMs are derivatives of the **LLaVA framework**:\n",
    "\n",
    "- **Vision Encoder**: Processes images into embeddings\n",
    "- **Linear Connection**: Maps vision embeddings to the language model's token space\n",
    "- **Language Model**: transformer decoder-only model that does next-token prediction\n",
    "\n",
    "For this connection to be effective, having a vision encoder already aligned with the language space is essential - this is where CLIP's vision-text alignment becomes important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "\n",
    "# Load a CLIP model - This might take a while as it has to be downloaded.\n",
    "clip_model, _, preprocess_clip = open_clip.create_model_and_transforms('ViT-L-14', pretrained='laion2b_s32b_b82k')\n",
    "clip_model.eval()\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('ViT-L-14')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot classification\n",
    "\n",
    "You can do lots of fun stuff with CLIP models: once you have a common embedding space for image and text, you can measure distances between them. \n",
    "Hopefully, this distance is correlated to the vague idea of semantic content, and the closest two embeddings are in the latent space, the more they refer to similar content. \n",
    "You can for instance perform zero-shot classification! I.e, classification without further fine-tuning any model.\n",
    "\n",
    "Let's suppose you have two classes ['A', 'B'] and an image I that you want to classify, and CLIP(.) the CLIP operator, computing the CLIP embedding.\n",
    "\n",
    "Here's how zero-shot classification works:\n",
    "\n",
    "1. First, you compute the CLIP embedding of your image I: CLIP_img = CLIP(I)\n",
    "\n",
    "2. Then, you compute the CLIP embeddings of text prompts for each class:\n",
    "   - CLIP_A = CLIP(\"This is an image of class A\")\n",
    "   - CLIP_B = CLIP(\"This is an image of class B\")\n",
    "\n",
    "3. Finally, you compute the similarity (usually cosine similarity) between your image embedding and each class embedding:\n",
    "   - sim_A = cosine_similarity(CLIP_img, CLIP_A)\n",
    "   - sim_B = cosine_similarity(CLIP_img, CLIP_B)\n",
    "\n",
    "4. The class with the highest similarity score is your prediction!\n",
    "\n",
    "What makes this \"zero-shot\" is that you never had to train the model on your specific classification task. The model learned general visual-language relationships during its pre-training, and you're leveraging that knowledge to perform a new task without any additional training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brittany = Image.open(\"assets/relief_map_of_france_bretagne.jpg\")\n",
    "italy = Image.open(\"assets/relief_map_of_italy.jpg\")\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(brittany)\n",
    "plt.title(\"Brittany\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(italy)\n",
    "plt.title(\"Italy\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# And here is how to encode images and texts:\n",
    "#Image:\n",
    "brit_encoded = preprocess_clip(brittany)\n",
    "it_encoded = preprocess_clip(italy)\n",
    "\n",
    "texts = [\"A sentence to encode\"]\n",
    "texts_tokens = tokenizer(texts)\n",
    "console.print(\"Texts tokens: \", texts_tokens.shape)\n",
    "console.print(\"preprocessed image: \", brit_encoded.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Question 2.1_** Observe the shapes of images and text \"tokens\", i.e preprocessed before feeding the CLIP model. What do you observe?\n",
    "\n",
    "- **For the image**: it is automatically reshaped, cropped etc... → Be careful with what you feed in; this might remove important information.\n",
    "- **For the text**: Try to tokenize several sentences. All token sequences are of size 77. They all start and end with a given token id - SOS and EOS.\n",
    "\n",
    "Indeed in the CLIP model, text encoder was using absolute positional embeddings — which fixed the maximum sequence to 77. \n",
    "In addition, contrary to other models such as BERT, CLIP encoder was thought as a decoder only model (like GPTs are) and we use the EOS token as the encoding for the full sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocess_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "dir(clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clip_model.positional_embedding.shape)\n",
    "print(clip_model.token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_encoded = clip_model.encode_image(it_encoded.unsqueeze(0))\n",
    "brit_encoded = clip_model.encode_image(brit_encoded.unsqueeze(0))\n",
    "texts_encoded = clip_model.encode_text(texts_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(f\"Image features shape: {brit_encoded.shape}\")\n",
    "console.print(f\"Text features shape: {texts_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, embeddings of images and text have the same dimensions: image and text embeddings have been trained to live in the same space!\n",
    "\n",
    "> **_Question 2.2_** Implement a classification algorithm using these maps images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(x, y):\n",
    "    \"\"\"Compute cosine similarity between two tensors after normalization.\"\"\"\n",
    "    x = x / x.norm(dim=-1, keepdim=True)\n",
    "    y = y / y.norm(dim=-1, keepdim=True)\n",
    "    return x @ y.T\n",
    "\n",
    "# You can solve this kind of \"easy\" classification tasks\n",
    "texts = [\"An image of a map\", \"An image of a volcano\"]\n",
    "texts_tokens = tokenizer(texts)\n",
    "texts_encoded = clip_model.encode_text(texts_tokens)\n",
    "\n",
    "# Compute the cosine similarity between the image and text embeddings\n",
    "brit_similarity = cosine(brit_encoded, texts_encoded)\n",
    "it_similarity = cosine(it_encoded, texts_encoded)\n",
    "\n",
    "table = Table(title=\"Cosine Similarity\")\n",
    "table.add_column(\"Image\")\n",
    "table.add_column(\"Map\", justify=\"right\")\n",
    "table.add_column(\"Volcano\", justify=\"right\")\n",
    "table.add_row(\"Brittany\", f\"{brit_similarity[0][0].item():.3f}\", f\"{brit_similarity[0][1].item():.3f}\")\n",
    "table.add_row(\"Italy\", f\"{it_similarity[0][0].item():.3f}\", f\"{it_similarity[0][1].item():.3f}\")\n",
    "\n",
    "console.print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But it can work on more difficult tasks too...\n",
    "texts = [\"An image of a map of Brittany\", \"An image of a map of Italy\"]\n",
    "texts_tokens = tokenizer(texts)\n",
    "texts_encoded = clip_model.encode_text(texts_tokens)\n",
    "\n",
    "brit_similarity = cosine(brit_encoded, texts_encoded)\n",
    "it_similarity = cosine(it_encoded, texts_encoded)\n",
    "\n",
    "table = Table(title=\"Cosine Similarity\")\n",
    "table.add_column(\"Image\")\n",
    "table.add_column(\"Map of Brittany\", justify=\"right\")\n",
    "table.add_column(\"Map of Italy\", justify=\"right\")\n",
    "table.add_row(\"Brittany\", f\"{brit_similarity[0][0].item():.3f}\", f\"{brit_similarity[0][1].item():.3f}\")\n",
    "table.add_row(\"Italy\", f\"{it_similarity[0][0].item():.3f}\", f\"{it_similarity[0][1].item():.3f}\")\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does that work with pathology, now? \n",
    "Images are super different, and a lot of the structure in natural images is different here:\n",
    "- **No polarity** (up/down, right/left)\n",
    "- **No depth** (no forefront/backfront)\n",
    "- Images are **not object-centric**\n",
    "- ...\n",
    "\n",
    "Let's have a look at a dataset of pairs of histopathologic images / captions gathered on Twitter (where a lot of pedagogical content was shared) named [OpenPath](https://paperswithcode.com/paper/leveraging-medical-twitter-to-build-a-visual)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('assets/captions_openpath.json', 'r') as f:\n",
    "    captions = json.load(f)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "for idx, (ax, item) in enumerate(zip(axes, captions)):\n",
    "    img = Image.open(f'assets/{item[\"filename\"]}')\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(item['caption'], wrap=True)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display captions in rich console\n",
    "console = Console()\n",
    "for item in captions:\n",
    "    text = Text(item['caption'])\n",
    "    panel = Panel(text, title=f\"Image: {item['filename']}\", border_style=\"green\")\n",
    "    console.print(panel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the alignment between one of these images and text sentences, starting with its own caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode an image and its caption\n",
    "im_id = 0\n",
    "tokenizer = open_clip.get_tokenizer('ViT-L-14')\n",
    "img = Image.open(Path('assets') / captions[im_id]['filename'])\n",
    "text = captions[im_id]['caption']\n",
    "img_clip = preprocess_clip(img).unsqueeze(0)\n",
    "text_clip = tokenizer([text])\n",
    "\n",
    "img_features = clip_model.encode_image(img_clip)\n",
    "text_features = clip_model.encode_text(text_clip)\n",
    "\n",
    "cosine_similarity = cosine(img_features, text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the similarity between the image and text\n",
    "cosine_similarity = cosine(img_features, text_features)\n",
    "console.print(f\"Cosine similarity between image and text: {cosine_similarity.item()}\")\n",
    "\n",
    "unrelated_text = tokenizer([\"I drink the cat\"])\n",
    "unrelated_text_features = clip_model.encode_text(unrelated_text)\n",
    "unrelated_cosine_similarity = cosine(img_features, unrelated_text_features)\n",
    "console.print(f\"Cosine similarity between image and unrelated text: {unrelated_cosine_similarity.item()}\")\n",
    "\n",
    "somewhat_related_text = tokenizer([\"Histopathology image\"])\n",
    "somewhat_related_text_features = clip_model.encode_text(somewhat_related_text)\n",
    "somewhat_related_cosine_similarity = img_features @ somewhat_related_text_features.T / (torch.norm(img_features, dim=1) * torch.norm(somewhat_related_text_features, dim=1))\n",
    "console.print(f\"Cosine similarity between image and somewhat related text: {somewhat_related_cosine_similarity.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Question 2.3_** Comment on these results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the general CLIP embedding model definitely can separate histopathology images from other domains, but lacks discrimination power inside the domain itself.\n",
    "Indeed, a vague caption \"histopathology Image\" is closer to the image, in the CLIP embedding space, than the real caption. \n",
    "\n",
    "Let's look at another example: some tiles extracted from [the TCGA](https://portal.gdc.cancer.gov/) (one of the biggest public repositories of slides).\n",
    "Some contain tumor cells, others don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_tiles = glob('assets/tumor_tiles/*.png')\n",
    "stroma_tiles = glob('assets/stroma/*.png')\n",
    "\n",
    "def create_image_grid(image_paths, grid_size=(2, 2)):\n",
    "    \"\"\"Creates a grid of images using einops.\"\"\"\n",
    "    h_grid, w_grid = grid_size\n",
    "    images_np = [np.array(Image.open(p)) for p in image_paths[:h_grid * w_grid]]\n",
    "    return Image.fromarray(\n",
    "        einops.rearrange(\n",
    "            np.stack(images_np),\n",
    "            '(h_grid w_grid) h w c -> (h_grid h) (w_grid w) c',\n",
    "            h_grid=h_grid\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Create the figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].imshow(create_image_grid(tumor_tiles))\n",
    "axes[0].set_title('Tumor Tiles')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(create_image_grid(stroma_tiles))\n",
    "axes[1].set_title('Stroma Tiles')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Question 2.4_** Implement the classifiation algorithm between tiles with healthy tissue and tumor cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the text prompts.\n",
    "text_prompts = [\"A histopathological tile with tumour cells\", \"An histopathological tile without tumour cells\"]\n",
    "text_clip = tokenizer(text_prompts)\n",
    "\n",
    "tumor_imgs = [Image.open(p) for p in tumor_tiles]\n",
    "stroma_imgs = [Image.open(p) for p in stroma_tiles]\n",
    "\n",
    "tumor_img_clip = torch.stack([preprocess_clip(img) for img in tumor_imgs])\n",
    "stroma_img_clip = torch.stack([preprocess_clip(img) for img in stroma_imgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tumor_embeddings = clip_model.encode_image(tumor_img_clip)\n",
    "    stroma_embeddings = clip_model.encode_image(stroma_img_clip)\n",
    "    text_embeddings = clip_model.encode_text(text_clip)\n",
    "\n",
    "tumor_logits = cosine(tumor_embeddings, text_embeddings)\n",
    "stroma_logits = cosine(stroma_embeddings, text_embeddings)\n",
    "\n",
    "table = Table(title=\"CLIP Classification Logits\")\n",
    "table.add_column(\"Image Sample\", justify=\"left\", style=\"cyan\", no_wrap=True)\n",
    "table.add_column(f\"Logit: '{text_prompts[0]}'\", justify=\"right\")\n",
    "table.add_column(f\"Logit: '{text_prompts[1]}'\", justify=\"right\")\n",
    "\n",
    "tumor_probs = tumor_logits.softmax(dim=-1)\n",
    "stroma_probs = stroma_logits.softmax(dim=-1)\n",
    "\n",
    "for i, (logits, p) in enumerate(zip(tumor_probs, tumor_tiles)):\n",
    "    l1, l2 = logits\n",
    "    sample_name = f\"tumor_{i} ({Path(p).name})\"\n",
    "    table.add_row(sample_name, f\"{l1:.4f}\", f\"{l2:.4f}\")\n",
    "\n",
    "table.add_section()\n",
    "for i, (logits, p) in enumerate(zip(stroma_probs, stroma_tiles)):\n",
    "    l1, l2 = logits\n",
    "    sample_name = f\"stroma_{i} ({Path(p).name})\"\n",
    "    table.add_row(sample_name, f\"{l1:.4f}\", f\"{l2:.4f}\")\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_preds = tumor_probs.argmax(dim=-1)\n",
    "stroma_preds = stroma_probs.argmax(dim=-1)\n",
    "\n",
    "tumor_accuracy = (tumor_preds == 0).float().mean()\n",
    "stroma_accuracy = (stroma_preds == 1).float().mean()\n",
    "\n",
    "console.print(f\"Tumor tile classification accuracy: {tumor_accuracy.item():.2f}\")\n",
    "console.print(f\"Stroma tile classification accuracy: {stroma_accuracy.item():.2f}\")\n",
    "console.print(f\"Overall classification accuracy: {(tumor_accuracy + stroma_accuracy) / 2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Question 2.5_**: Experiment a little with changing the framing of the sentences for these classification tasks. What could you conclude regarding the evaluation on the zero-shot task ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "The general domain CLIP models fail to discriminate pathology images well. The field would therefore benefit from a CLIP model trained on pathology images! However, contrary to the general domain where image/caption pairs fill the internet, in this specialized domain, such data is harder to find!\n",
    "\n",
    "Other research efforts have gathered more of these pairs using youtube ([QUILT](https://arxiv.org/abs/2306.11207)), medical publications, books ([CONCH](https://www.nature.com/articles/s41591-024-02856-4)) and twitter ([PLIP](https://www.nature.com/articles/s41591-023-02504-3)) etc...\n",
    "This allowed impressive improvements in the zero-shot capabilities of these models... But can you spot the issue with these datasets ? \n",
    "\n",
    "> **_Question 2.6_** Have a look at the images from openpath and list the issues with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "As you can see, captions are very detailed but images quality may vary a lot:\n",
    "- Magnification is not consistent\n",
    "- There are numerous artifacts, texts, pen strokes, etc.\n",
    "- This is indeed targeting a human audience (therefore, the pen marks etc...)\n",
    "- Many image components are not annotated because too obvious (e.g., presence of lymphocytes in the right image)\n",
    "\n",
    "> What would you propose to do to improve the dataset?\n",
    ">\n",
    "> As a reminder, we have at our disposal:\n",
    "> - A base image/caption dataset, but with low quality images and captions not exhaustive\n",
    "> - An enormous amount of unlabeled tiles data\n",
    "> - general LLM/VLLM models that are very good at text-based tasks but make errors in the pathology space\n",
    "> - Reasonably well performing CLIP models\n",
    "> - WSI and accompanying pathology reports describing findings pathologist have made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "## One solution: PathGen\n",
    "\n",
    "[PathGen](https://arxiv.org/abs/2407.00203) is a paper that appeared at ICLR 2025. \n",
    "\n",
    "The authors propose to use a hive of VLM/LLM models to build a dataset of tile/caption pairs of high quality.\n",
    "The hope is that all these models can complement each other and make use of the available *seed* datasets.\n",
    "\n",
    "The basic idea is to mine a lot of high-quality tiles, at similar magnification, directly from the WSIs, and caption them using VLLM.\n",
    "Let's see how they do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Regarding tile mining, this need a small digression: in addition to being able to do zero-shot classification, you can do dense cross-modol retrieval also!\n",
    "\n",
    "**Dense Retrieval**: Search in a bank of images for the one that would be best described by a given text prompt - Or using the same modality: that would be closer to a given probe image.\n",
    "\n",
    "> **_Question 2.7_** How would you implement that? Write that down using the previous tiles: \"I am searching for all the tiles that contain tumorous tissue.\"\n",
    "> Try implementing that on the embeddings i made you download in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_embeddings = torch.vstack([tumor_embeddings, stroma_embeddings])\n",
    "tiles_path = [Path(p) for p in tumor_tiles + stroma_tiles]\n",
    "text_probe = tokenizer([\"An H&E image with tumor cells\"])\n",
    "# An H&E image with a low density of cells\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_probe_embeddings = clip_model.encode_text(text_probe)\n",
    "\n",
    "similarity = cosine(tiles_embeddings, text_probe_embeddings).squeeze()\n",
    "sorted_indices = torch.argsort(similarity, descending=True)\n",
    "\n",
    "best_idx, worst_idx = sorted_indices[0], sorted_indices[-1]\n",
    "best_score, worst_score = similarity[best_idx].item(), similarity[worst_idx].item()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].imshow(Image.open(tiles_path[best_idx]))\n",
    "axes[0].set_title(f'Best match\\nScore: {best_score:.3f}', \n",
    "                  color='green')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(Image.open(tiles_path[worst_idx]))\n",
    "axes[1].set_title(f'Worst match\\nScore: {worst_score:.3f}', \n",
    "                  color='red')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 4\n",
    "descending = True \n",
    "indices_to_show = sorted_indices[-top_k:] if not descending else sorted_indices[:top_k]\n",
    "scores_to_show = similarity[indices_to_show].squeeze()\n",
    "\n",
    "fig, axes = plt.subplots(1, top_k, figsize=(15, 4))\n",
    "for i, (idx, score) in enumerate(zip(indices_to_show, scores_to_show)):\n",
    "    axes[i].imshow(Image.open(tiles_path[idx]))\n",
    "    axes[i].set_title(f'Rank {len(tiles_path)-top_k+i+1 if not descending else i+1}\\nScore: {score:.3f}', \n",
    "                     color='red' if not descending else 'green')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "he idea of PathGen is to build captioning models (VLLMs) using the available multimodal datasets, \n",
    "in order to label a high quality dataset of tiles, extracted directly from WSIs.\n",
    "\n",
    "## Step 1: Caption Enhancement with Vision LLMs\n",
    "\n",
    "The first step is to improve the current image/captions pairs datasets (like PLIP, OpenPath, etc...).\n",
    "One of their drawbacks was their 'non-exhaustivity', i.e. these captions, coming from an educational source, assumed a lot of a-priori knowledge from the reader \n",
    "(for instance, it is not often formulated that the image is a pathology image).\n",
    "This idea is therefore to complete these captions using a general domain VLLM like GPT-4o, that can describe low level features with reasonable efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Note_**: \n",
    "> Because running an LLM on a personnal laptop is often infeasible (in google Colab as well), I had to find an alternative way of doing so. \n",
    "> If you want to do so yourself, you could either:\n",
    "> - Use endpoints of LLM providers (MistralAI, OpenAI, Cohere) and a *personal* API key\n",
    "> - Use the serverless inference services proposed by company like [HuggingFace](https://huggingface.co/learn/cookbook/enterprise_hub_serverless_inference_api) or [TogetherAI](https://www.together.ai/) etc. The service is similar to the one offered by LLM providers, but extends to open-source models. You do not have to bother about anything regarding your calls, service extends as a function of your needs (constrained by your token-per-minute limit). For personal use-case I would recommend that.\n",
    "> - Use cloud providers that allow you to deploy model on a distant device. Azure, VertexAI (google), Amazon, and even HuggingFace ([InferenceProvider](https://endpoints.huggingface.co/)) you do not pay per call but per hours of activity. \n",
    "> \n",
    "> For this course, I tried the last solution as it seemed to be the only one available to *serve* an LLM to a group of people - Without investing a lot of time into it, this seems an unstable solution. The GPU I rent kept getting overloaded and the endpoint crashed with only a few requests.\n",
    "> \n",
    "> I therefore put in place an intermediate solution: [this little huggingface space](https://huggingface.co/spaces/trizard/ai4health/tree/main), serving a small python server. Under the hood, this python server just calls one of the LLM provider (MistralAI here, cocorico) using my API key, but keeping it secure (By the way, never ever share such an API key).\n",
    "> \n",
    "> I would recommend doing so for any idea/hack using LLM that you would like to serve to a small group of people, for testing or educational (or fun) purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# And here is a simple class using the request package to submit request to the HF-space server\n",
    "class SimpleChatClient:\n",
    "    def __init__(self, endpoint, model=\"mistral-small-latest\"): #Please dont change that to **large**, I am paying for the calls :'(\n",
    "        self.endpoint = endpoint\n",
    "        self.model = model\n",
    "\n",
    "    def encode_image_to_base64(self, image_path, max_size=256, quality=30):\n",
    "        \"\"\"\n",
    "        I'm here aggressively reducing the image quality here in the hope that everyone could use this endpoint without crashing it.\n",
    "        You could just encode in base64 the image, though, in other circumstances.\n",
    "\n",
    "        Base64 encoding is a good way to send information through HTTP requests: byte encoding \n",
    "        may contain special characters that would break the request, base64 encodes chunks of \n",
    "        6 bites at a time, with 64 possible values, all contained in the standard, safe ASCII set.\n",
    "\n",
    "        Of course, the image is then decoded on the other side -i.e. in MistralAI's servers.\n",
    "        \"\"\"\n",
    "        with Image.open(image_path) as img:\n",
    "            if img.mode == 'RGBA':\n",
    "                img = img.convert('RGB')\n",
    "            ratio = max_size / max(img.size)\n",
    "            if ratio < 1:\n",
    "                new_size = tuple(int(dim * ratio) for dim in img.size)\n",
    "                img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "            buffer = io.BytesIO()\n",
    "            img.save(buffer, format='JPEG', quality=quality, optimize=True)\n",
    "            encoded_string = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "        return f\"data:image/jpeg;base64,{encoded_string}\"\n",
    "\n",
    "    def chat(self, text, image_path=None, max_tokens=150):\n",
    "        # Sends an http request to the proxy server.\n",
    "        messages = []\n",
    "        content = []\n",
    "        if text:\n",
    "            content.append({\"type\": \"text\", \"text\": text})\n",
    "        if image_path:\n",
    "            base64_image = self.encode_image_to_base64(image_path)\n",
    "            content.append({\"type\": \"image_url\", \"image_url\": base64_image})\n",
    "        messages.append({\"role\": \"user\", \"content\": content})\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "        response = requests.post(\n",
    "            self.endpoint,\n",
    "            json=payload,\n",
    "            headers=headers,\n",
    "            timeout=60\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    ">   **_Question 2.8_** Here comes a bit of prompt engineering: design a prompt to improve the caption of an image.\n",
    ">   If you want help creating a good prompt, there are online tools that do just that. \n",
    ">   For instance, Anthropic proposes this service (but you would need to have an API key). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_description_prompt(caption):\n",
    "    prompt = f\"\"\"\n",
    "    You are a histopathologic expert. Given an H&E image, your task is to extend an incomplete image description. Focus on the details you can see in the image.\n",
    "\n",
    "    Incomplete description: {caption}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "# If you want help creating a good prompt, there are online tools that do just that.\n",
    "# For instance, anthropic proposes this service (but you would need to have an API key).\n",
    "# Here is the prompt he created given mine:\n",
    "\n",
    "def get_description_prompt_anthropic(caption):\n",
    "    \"\"\"\n",
    "    Creates a detailed prompt for a vision model, based on Anthropic's template,\n",
    "    to extend a histopathology image caption.\n",
    "    Your answer will start with the original incomplete description, then seamlessly integrate your additional observations and details.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a histopathologic expert tasked with extending an incomplete description of an H&E (Hematoxylin and Eosin) stained tissue image. \n",
    "    Your goal is to provide a detailed and accurate description of the image, focusing on the histological features and any potential pathological findings.\n",
    "\n",
    "First, you are presented with an incomplete description of the H&E image:\n",
    "\n",
    "Incomplete description:\n",
    "{caption}\n",
    "\n",
    "Next, you will be shown the H&E image.\n",
    "\n",
    "Carefully examine the H&E image, paying attention to the following aspects:\n",
    "1. Tissue type and architecture\n",
    "2. Cellular composition and distribution\n",
    "3. Nuclear characteristics (size, shape, chromatin pattern)\n",
    "4. Cytoplasmic features\n",
    "5. Presence of any abnormal structures or cellular arrangements\n",
    "6. Stromal components\n",
    "7. Vascular elements\n",
    "8. Any signs of inflammation, necrosis, or other pathological processes\n",
    "\n",
    "Based on your observations, extend the incomplete description by adding relevant details about the histological features you can see in the image. Be specific and use appropriate medical terminology. If you notice any potential pathological findings, describe them objectively without making a definitive diagnosis.\n",
    "\n",
    "Start with the original incomplete description, then seamlessly integrate your additional observations and details.]\n",
    "\n",
    "Remember to maintain a professional and objective tone throughout your description. Focus on what you can directly observe in the image, and avoid speculating about diagnoses or clinical implications unless they are clearly evident from the histological features.\n",
    "Answer ONLY with your extended description, nothing else.\"\"\"\n",
    "    return prompt\n",
    "\n",
    "client = SimpleChatClient(\"https://trizard-ai4health.hf.space/chat\")\n",
    "\n",
    "for item in captions:\n",
    "    image_path = Path('assets') / item['filename']\n",
    "    \n",
    "    new_caption = client.chat(get_description_prompt_anthropic(item['caption']), image_path=image_path)\n",
    "    print(new_caption)\n",
    "    \n",
    "    panel_content = (\n",
    "        f\"[bold]Original Caption:[/bold]\\n{item['caption']}\\n\\n\"\n",
    "        f\"[bold cyan]Augmented Caption:[/bold cyan]\\n{new_caption}\"\n",
    "    )\n",
    "    img = Image.open(Path('assets') / item['filename'])\n",
    "    console.print(Panel(panel_content, title=f\"Caption Augmentation for {item['filename']}\", border_style=\"blue\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 2: Fine-tune a **Revise VLLM**\n",
    "\n",
    "With this method of caption augmentation - the authors have build a dataset for fine-tuning a VLLM. \n",
    "The next step has therefore been to fine-tune a LLaVa model on it, creating a VLLM able to caption pathology tiles.\n",
    "\n",
    "We can therefore suppose in the rest of the tutorial that the Llava model we are using has extended capabilities in pathology. \n",
    "We will still use the general LLaVa-11B-Instruct because PathGen-LLaVa-desp would be too big to run locally on your machines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    " ### Step 3: Caption Revision Model\n",
    "\n",
    "PathGen-LlaVa-desp can describe pathology images better than general domain models. It is not however perfect. \n",
    "Improving the model could be done by improving the dataset - hard to do without additional expertise.\n",
    "\n",
    "The authors have bet on the training of a model able to **revise** and **correct** the captions made by PathGen-LlaVa-desp.\n",
    "\n",
    ">  **_Question 2.9_** How would you do that?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 4: Create a **revise** dataset and Fine-Tune a **Revise VLLM**\n",
    "\n",
    "To do that, we need a dataset composed of images paired with 2 captions: 1 correct and 1 incorrect.\n",
    "\n",
    "The idea here will be to degrade the LLaVa descriptions with known modifications: additions, deletions, modifications. This is a type of self-supervised learning led by the LLM (just like conventional contrastive learning in image processing perturbs a given image).\n",
    "\n",
    "We then \"just\" have to fine-tune an LLM on this dataset to get a \"Revise LLM\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's suppose that this LLama-3.2-11B has been trained to describe histopathological images.\n",
    "image_path = Path('assets') / captions[0]['filename']\n",
    "client = SimpleChatClient(\"https://trizard-ai4health.hf.space/chat\")\n",
    "completion = client.chat(\n",
    "    \"You are an expert in anatomopathology in a hospital. You are given an H&E image of a tissue sample: describe it in detail, focusing on the histological features and any potential pathological findings. You will answer only with your description of the image.\",\n",
    "    image_path=image_path\n",
    ")\n",
    "image_description = completion\n",
    "\n",
    "print(image_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try designing querying a model available on the huggingface hub and write the pipeline to create this dataset.\n",
    "prompts = {\n",
    "    \"addition\": lambda x: f\"You are tasked to selectively degrade an image description by adding a the description of a new visual detail (and only that!). You will answer only with the new, degraded description. \\n\\n Original description: {x}\",\n",
    "    \"deletion\": lambda x: f\"You are tasked to selectively degrade an image description by removing the description of a visual detail from it (and only that!). You will answer only with the new, degraded description. \\n\\n Original description: {x}\",\n",
    "    \"modification\": lambda x: f\"You are tasked to selectively degrade an image description by modifying the description of a visual detail from it (and only that!) - modify it such that it mainly changes the meaning of its description. You will answer only with the new, degraded description. \\n\\n Original description: {x}\",\n",
    "}\n",
    "\n",
    "def degrade_description(image_description):\n",
    "    # Randomly select a degradation type\n",
    "    deg_type = random.choice(list(prompts.keys()))\n",
    "    prompt = prompts[deg_type](image_description)\n",
    "\n",
    "    completion = client.chat(prompt, image_path=image_path)\n",
    "    return deg_type, completion\n",
    "\n",
    "deg_type, degraded_description = degrade_description(image_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(Panel(\n",
    "    image_description,\n",
    "    title=\"Original Description\",\n",
    "    border_style=\"green\"\n",
    "))\n",
    "\n",
    "console.print(Panel(\n",
    "    degraded_description,\n",
    "    title=f\"Description degraded with [red]{deg_type}[/red]\",\n",
    "    border_style=\"green\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "### Step 5: Leveraging the TCGA Dataset\n",
    "\n",
    "The TCGA dataset is a public resource of Whole Slide Images. \n",
    "It contains not only WSI, but also (and mostly) other modalities, such as genomic, transcriptomic, and even text. \n",
    "Authors gathered 7300 WSI paired with their pathology reports - below is an example for the slide used in the previous notebook.\n",
    "\n",
    "We therefore now possess a **description VLLM**, a **Revise VLLM** and we also suppose that we dispose of a **Summarize LLM**, trained to summarized too long descriptions.\n",
    "\n",
    "#### Dataset Construction Pipeline\n",
    "\n",
    "We will now design a pipeline to create a high quality dataset of image/caption pairs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "> **_Question 2.10_**\n",
    "> Suppposing you have all these new tools, propose a pipeline to create a high quality dataset of image/caption pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathology_report = \"\"\"\n",
    "Sections show an infiltrating mammary carcinoma characterized by poor tubule formation, intermediate nuclear grade and high mitotic activity. \n",
    "The tumor cells infiltrate as sheets, single file, alveolar nests and occasional larger nests.\n",
    "The tumor has a tendency to infiltrate around existing ductal structures and focally formd targetoid lesions around this. \n",
    "Focal early necrosis is noted. \n",
    "There is a desmoplastic stromal response.\"\"\"\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = SimpleChatClient(\"https://trizard-ai4health.hf.space/chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Tile Sampling Strategy\n",
    "\n",
    "The idea will simply be to dig into the WSIs we have, extract the most relevant and diverse tiles, \n",
    "and then make them pass through the different VLM we have, for description, revision and summarization.\n",
    "\n",
    "#### Prompt-based Extraction: first using pathology report!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = np.load('assets/embeddings_tiles.npy')\n",
    "\n",
    "prompts_from_report = pathology_report.split('.')[:-1]\n",
    "tokenized_prompts = tokenizer(prompts_from_report)\n",
    "prompts_report_embeddings = clip_model.encode_text(tokenized_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt-based extraction: using general model knowledge\n",
    "You can also try to distill a bit of general knowledge from an LLM into this tile selection phase. \n",
    "> Create prompts to use for dense retrieval using a CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GPT generated prompts:\n",
    "prompts_from_gpt = [client.chat(\"You are an expert in histopathology. You are task to describe one pattern that could be present in a breast cancer slide. Answer exclusively by giving an example pattern.\", max_tokens=10) for _ in range(3)]\n",
    "tokenized_prompts = tokenizer(prompts_from_gpt)\n",
    "prompts_gpt_embeddings = clip_model.encode_text(tokenized_prompts)\n",
    "\n",
    "prompts = torch.vstack([prompts_report_embeddings, prompts_gpt_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity = cosine(torch.tensor(tiles), torch.tensor(prompts))\n",
    "cosine_similarity = torch.max(cosine_similarity, dim=1).values\n",
    "first_128_indices = torch.argsort(cosine_similarity, descending=True)[:128]\n",
    "prompt_extracted_tiles = tiles[first_128_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 2. Diversity Considerations\n",
    "\n",
    "> What could be one caveat of sampling tiles using only prompt-based retrieval? \n",
    "> Implement an alternative approach for tile-sampling.\n",
    "\n",
    "The goal of this dataset is to screen tiles as diverse as possible (and, if possible, containing as much interesting features as possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# They augmented their tile dataset with a KMeans approach.\n",
    "# Each cluster being sampled uniformly.\n",
    "# Calculate number of clusters - square root of total patches\n",
    "n_clusters = int(math.sqrt(len(tiles)))\n",
    "print(f\"Using {n_clusters} clusters for {len(tiles)} total tiles\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "cluster_labels = kmeans.fit_predict(tiles)\n",
    "\n",
    "samples_per_cluster = 256 // n_clusters\n",
    "remaining_samples = 256 % n_clusters\n",
    "\n",
    "cluster_sampled_indices = []\n",
    "for cluster_idx in range(n_clusters):\n",
    "    cluster_indices = np.where(cluster_labels == cluster_idx)[0]\n",
    "    # Add one extra sample to some clusters if we have remaining samples\n",
    "    n_samples = samples_per_cluster + (1 if cluster_idx < remaining_samples else 0)\n",
    "    # We sample randomly from the cluster\n",
    "    if len(cluster_indices) > n_samples:\n",
    "        sampled_indices = np.random.choice(cluster_indices, size=n_samples, replace=False)\n",
    "        cluster_sampled_indices.extend(sampled_indices)\n",
    "    else:\n",
    "        # This is highly unlikely at 10 or 20x\n",
    "        cluster_sampled_indices.extend(cluster_indices)\n",
    "\n",
    "cluster_sampled_indices = torch.tensor(cluster_sampled_indices)\n",
    "cluster_extracted_tiles = tiles[cluster_sampled_indices]\n",
    "\n",
    "# Combine with prompt-based extracted tiles (which we got earlier)\n",
    "# Remove any duplicates that might exist between the two methods\n",
    "combined_indices = torch.concat([first_128_indices, cluster_sampled_indices])\n",
    "unique_indices = torch.unique(combined_indices)\n",
    "\n",
    "# # Get final selection of tiles\n",
    "kmeans_extracted_tiles = tiles[unique_indices]\n",
    "final_tiles = torch.concat([torch.tensor(prompt_extracted_tiles), torch.tensor(kmeans_extracted_tiles)])\n",
    "print(f\"Final number of extracted tiles: {len(final_tiles)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Finally: Caption Generation\n",
    "\n",
    "Step 2 in the process is to use the description/revise/summarize VLMs that have been trained before in order to generate new captions for this whole dataset.\n",
    "Doing so, they gather >1.6M of high-quality tile/caption pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, they finally trained a CLIP model on this new dataset, **PathGen-CLIP-L** ! The resulting model shows unprecedented accuracies in many settings.\n",
    "Let's try its capabilities with the tasks we tried to tackle before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, _, preprocess_clip = open_clip.create_model_and_transforms('ViT-L-14', pretrained='assets/pathgen-clip-l.pt')\n",
    "clip_model.eval()\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('ViT-L-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the text prompts.\n",
    "text_prompts = [\"A histopathological tile with tumour cells\", \"An histopathological tile without tumour cells\"]\n",
    "text_clip = tokenizer(text_prompts)\n",
    "\n",
    "tumor_imgs = [Image.open(p) for p in tumor_tiles]\n",
    "stroma_imgs = [Image.open(p) for p in stroma_tiles]\n",
    "\n",
    "tumor_img_clip = torch.stack([preprocess_clip(img) for img in tumor_imgs])\n",
    "stroma_img_clip = torch.stack([preprocess_clip(img) for img in stroma_imgs])\n",
    "\n",
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    tumor_embeddings = clip_model.encode_image(tumor_img_clip)\n",
    "    stroma_embeddings = clip_model.encode_image(stroma_img_clip)\n",
    "    text_embeddings = clip_model.encode_text(text_clip)\n",
    "\n",
    "# Compute logits using our simple cosine function\n",
    "tumor_logits = cosine(tumor_embeddings, text_embeddings)\n",
    "stroma_logits = cosine(stroma_embeddings, text_embeddings)\n",
    "\n",
    "table = Table(title=\"CLIP Classification Logits\")\n",
    "table.add_column(\"Image Sample\", justify=\"left\", style=\"cyan\", no_wrap=True)\n",
    "table.add_column(f\"Logit: '{text_prompts[0]}'\", justify=\"right\")\n",
    "table.add_column(f\"Logit: '{text_prompts[1]}'\", justify=\"right\")\n",
    "\n",
    "tumor_probs = tumor_logits.softmax(dim=-1)\n",
    "stroma_probs = stroma_logits.softmax(dim=-1)\n",
    "\n",
    "for i, (logits, p) in enumerate(zip(tumor_probs, tumor_tiles)):\n",
    "    l1, l2 = logits\n",
    "    sample_name = f\"tumor_{i} ({Path(p).name})\"\n",
    "    table.add_row(sample_name, f\"{l1:.4f}\", f\"{l2:.4f}\")\n",
    "\n",
    "table.add_section()\n",
    "for i, (logits, p) in enumerate(zip(stroma_probs, stroma_tiles)):\n",
    "    l1, l2 = logits\n",
    "    sample_name = f\"stroma_{i} ({Path(p).name})\"\n",
    "    table.add_row(sample_name, f\"{l1:.4f}\", f\"{l2:.4f}\")\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_preds = tumor_probs.argmax(dim=-1)\n",
    "stroma_preds = stroma_probs.argmax(dim=-1)\n",
    "\n",
    "tumor_accuracy = (tumor_preds == 0).float().mean()\n",
    "stroma_accuracy = (stroma_preds == 1).float().mean()\n",
    "\n",
    "console.print(f\"Tumor tile classification accuracy: {tumor_accuracy.item():.2f}\")\n",
    "console.print(f\"Stroma tile classification accuracy: {stroma_accuracy.item():.2f}\")\n",
    "console.print(f\"Overall classification accuracy: {(tumor_accuracy + stroma_accuracy) / 2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems already way better !\n",
    "\n",
    "**Overall, this paper illustrate the perspectives that offer these weak forms of model distillation. General LLM/VLM often allow to greatly extend seed datasets, using very few guidance and supervision -Here, for instance, human input was to focus the LLM on undescribed details of the image + was the choice of the caption degradation types.**\n",
    "\n",
    "> **_Question 2.11_** However, this strategy has limitations.\n",
    "> - Author call this pipeline \"agentic\". What do you think of this naming ? Do you find it appropriate here ? \n",
    "> - Why would they restrain themselves to a mere 1.6M tile/caption pairs, when they could create tens of millions ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
