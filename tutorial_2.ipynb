{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import os\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.text import Text\n",
    "from rich.table import Table\n",
    "import numpy as np\n",
    "import einops\n",
    "from glob import glob\n",
    "from huggingface_hub import InferenceClient\n",
    "import openai\n",
    "import base64\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "import math\n",
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "from openai import OpenAI\n",
    "\n",
    "console = Console()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# PathGen Tutorial: AI for Pathology\n",
    "\n",
    "This tutorial demonstrates the use of **PathGen-CLIP**, a specialized CLIP model for histopathology images, and explores the \"agentic\" pipeline for creating high-quality image-caption datasets in the pathology domain.\n",
    "\n",
    "## Background: CLIP Models and Vision-Language Alignment\n",
    "\n",
    "**CLIP (Contrastive Language-Image Pre-training)** models and their derivatives are Vision-Language Models (VLMs) that align vision and text modalities. Introduced by the [seminal work of Radford et al.](https://arxiv.org/abs/2103.00020), these models are trained using:\n",
    "\n",
    "- **Dataset**: Paired images and captions\n",
    "- **Objective**: Contrastive learning that brings closer the representations of positive pairs (image and its caption) while pushing apart negative pairs (image and captions of other images in the batch)\n",
    "\n",
    "This approach mirrors unimodal self-supervised contrastive learning but operates across modalities.\n",
    "\n",
    "## Applications in Visual Large Language Models\n",
    "\n",
    "CLIP models serve as the foundation for **VLLMs** (Visual Large Language Models) - systems that can process images and respond with text (like GPT-4o). Many  VLLMs are derivatives of the **LLaVA framework**:\n",
    "\n",
    "- **Vision Encoder**: Processes images into embeddings\n",
    "- **Linear Connection**: Maps vision embeddings to the language model's token space\n",
    "- **Language Model**: transformer decoder-only model that does next-token prediction\n",
    "\n",
    "For this connection to be effective, having a vision encoder already aligned with the language space is essential - this is where CLIP's vision-text alignment becomes important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "\n",
    "# Load a CLIP model - This might take a while as it has to be downloaded.\n",
    "clip_model, _, preprocess_clip = open_clip.create_model_and_transforms('ViT-L-14', pretrained='laion2b_s32b_b82k')\n",
    "clip_model.eval()\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('ViT-L-14')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot classification\n",
    "\n",
    "You can do lots of fun stuff with CLIP models: once you have a common embedding space for image and text, you can measure distances between them. \n",
    "Hopefully, this distance is correlated to the vague idea of semantic content, and the closest two embeddings are in the latent space, the more they refer to similar content. \n",
    "You can for instance perform zero-shot classification! I.e, classification without further fine-tuning any model.\n",
    "\n",
    "Let's suppose you have two classes ['A', 'B'] and an image I that you want to classify, and CLIP(.) the CLIP operator, computing the CLIP embedding.\n",
    "\n",
    "Here's how zero-shot classification works:\n",
    "\n",
    "1. First, you compute the CLIP embedding of your image I: CLIP_img = CLIP(I)\n",
    "\n",
    "2. Then, you compute the CLIP embeddings of text prompts for each class:\n",
    "   - CLIP_A = CLIP(\"This is an image of class A\")\n",
    "   - CLIP_B = CLIP(\"This is an image of class B\")\n",
    "\n",
    "3. Finally, you compute the similarity (usually cosine similarity) between your image embedding and each class embedding:\n",
    "   - sim_A = cosine_similarity(CLIP_img, CLIP_A)\n",
    "   - sim_B = cosine_similarity(CLIP_img, CLIP_B)\n",
    "\n",
    "4. The class with the highest similarity score is your prediction!\n",
    "\n",
    "What makes this \"zero-shot\" is that you never had to train the model on your specific classification task. The model learned general visual-language relationships during its pre-training, and you're leveraging that knowledge to perform a new task without any additional training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brittany = Image.open(\"assets/relief_map_of_france_bretagne.jpg\")\n",
    "italy = Image.open(\"assets/relief_map_of_italy.jpg\")\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(brittany)\n",
    "plt.title(\"Brittany\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(italy)\n",
    "plt.title(\"Italy\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# And here is how to encode images and texts:\n",
    "#Image:\n",
    "brit_encoded = preprocess_clip(brittany)\n",
    "it_encoded = preprocess_clip(italy)\n",
    "\n",
    "texts = [\"A sentence to encode\"]\n",
    "texts_tokens = tokenizer(texts)\n",
    "console.print(\"Texts tokens: \", texts_tokens.shape)\n",
    "console.print(\"preprocessed image: \", brit_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Question 2.1_** Observe the shapes of images and text \"tokens\", i.e preprocessed before feeding the CLIP model. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocess_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "dir(clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clip_model.positional_embedding.shape)\n",
    "print(clip_model.token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_encoded = clip_model.encode_image(it_encoded.unsqueeze(0))\n",
    "brit_encoded = clip_model.encode_image(brit_encoded.unsqueeze(0))\n",
    "texts_encoded = clip_model.encode_text(texts_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(f\"Image features shape: {brit_encoded.shape}\")\n",
    "console.print(f\"Text features shape: {texts_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, embeddings of images and text have the same dimensions: image and text embeddings have been trained to live in the same space!\n",
    "\n",
    "> **_Question 2.2_** Implement a classification algorithm using these maps images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does that work with pathology, now? \n",
    "Images are super different, and a lot of the structure in natural images is different here:\n",
    "- **No polarity** (up/down, right/left)\n",
    "- **No depth** (no forefront/backfront)\n",
    "- Images are **not object-centric**\n",
    "- ...\n",
    "\n",
    "Let's have a look at a dataset of pairs of histopathologic images / captions gathered on Twitter (where a lot of pedagogical content was shared) named [OpenPath](https://paperswithcode.com/paper/leveraging-medical-twitter-to-build-a-visual)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('assets/captions_openpath.json', 'r') as f:\n",
    "    captions = json.load(f)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "for idx, (ax, item) in enumerate(zip(axes, captions)):\n",
    "    img = Image.open(f'assets/{item[\"filename\"]}')\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(item['caption'], wrap=True)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display captions in rich console\n",
    "console = Console()\n",
    "for item in captions:\n",
    "    text = Text(item['caption'])\n",
    "    panel = Panel(text, title=f\"Image: {item['filename']}\", border_style=\"green\")\n",
    "    console.print(panel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the alignment between one of these images and text sentences, starting with its own caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode an image and its caption\n",
    "im_id = 0\n",
    "tokenizer = open_clip.get_tokenizer('ViT-L-14')\n",
    "img = Image.open(Path('assets') / captions[im_id]['filename'])\n",
    "text = captions[im_id]['caption']\n",
    "img_clip = preprocess_clip(img).unsqueeze(0)\n",
    "text_clip = tokenizer([text])\n",
    "\n",
    "img_features = clip_model.encode_image(img_clip)\n",
    "text_features = clip_model.encode_text(text_clip)\n",
    "\n",
    "cosine_similarity = cosine(img_features, text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the similarity between the image and text\n",
    "cosine_similarity = cosine(img_features, text_features)\n",
    "console.print(f\"Cosine similarity between image and text: {cosine_similarity.item()}\")\n",
    "\n",
    "unrelated_text = tokenizer([\"I drink the cat\"])\n",
    "unrelated_text_features = clip_model.encode_text(unrelated_text)\n",
    "unrelated_cosine_similarity = cosine(img_features, unrelated_text_features)\n",
    "console.print(f\"Cosine similarity between image and unrelated text: {unrelated_cosine_similarity.item()}\")\n",
    "\n",
    "somewhat_related_text = tokenizer([\"Histopathology image\"])\n",
    "somewhat_related_text_features = clip_model.encode_text(somewhat_related_text)\n",
    "somewhat_related_cosine_similarity = img_features @ somewhat_related_text_features.T / (torch.norm(img_features, dim=1) * torch.norm(somewhat_related_text_features, dim=1))\n",
    "console.print(f\"Cosine similarity between image and somewhat related text: {somewhat_related_cosine_similarity.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Question 2.3_** Comment on these results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another example: some tiles extracted from [the TCGA](https://portal.gdc.cancer.gov/) (one of the biggest public repositories of slides).\n",
    "Some contain tumor cells, others don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_tiles = glob('assets/tumor_tiles/*.png')\n",
    "stroma_tiles = glob('assets/stroma/*.png')\n",
    "\n",
    "def create_image_grid(image_paths, grid_size=(2, 2)):\n",
    "    \"\"\"Creates a grid of images using einops.\"\"\"\n",
    "    h_grid, w_grid = grid_size\n",
    "    images_np = [np.array(Image.open(p)) for p in image_paths[:h_grid * w_grid]]\n",
    "    return Image.fromarray(\n",
    "        einops.rearrange(\n",
    "            np.stack(images_np),\n",
    "            '(h_grid w_grid) h w c -> (h_grid h) (w_grid w) c',\n",
    "            h_grid=h_grid\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Create the figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].imshow(create_image_grid(tumor_tiles))\n",
    "axes[0].set_title('Tumor Tiles')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(create_image_grid(stroma_tiles))\n",
    "axes[1].set_title('Stroma Tiles')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Question 2.4_** Implement the classifiation algorithm between tiles with healthy tissue and tumor cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Question 2.5_**: Experiment a little with changing the framing of the sentences for these classification tasks. What could you conclude regarding the evaluation on the zero-shot task ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "The general domain CLIP models fail to discriminate pathology images well. The field would therefore benefit from a CLIP model trained on pathology images! However, contrary to the general domain where image/caption pairs fill the internet, in this specialized domain, such data is harder to find!\n",
    "\n",
    "Other research efforts have gathered more of these pairs using youtube ([QUILT](https://arxiv.org/abs/2306.11207)), medical publications, books ([CONCH](https://www.nature.com/articles/s41591-024-02856-4)) and twitter ([PLIP](https://www.nature.com/articles/s41591-023-02504-3)) etc...\n",
    "This allowed impressive improvements in the zero-shot capabilities of these models... But can you spot the issue with these datasets ? \n",
    "\n",
    "> **_Question 2.6_** Have a look at the images from openpath and list the issues with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "> What would you propose to do to improve the dataset?\n",
    ">\n",
    "> As a reminder, we have at our disposal:\n",
    "> - A base image/caption dataset, but with low quality images and captions not exhaustive\n",
    "> - An enormous amount of unlabeled tiles data\n",
    "> - general LLM/VLLM models that are very good at text-based tasks but make errors in the pathology space\n",
    "> - Reasonably well performing CLIP models\n",
    "> - WSI and accompanying pathology reports describing findings pathologist have made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "## One solution: PathGen\n",
    "\n",
    "[PathGen](https://arxiv.org/abs/2407.00203) is a paper that appeared at ICLR 2025. \n",
    "\n",
    "The authors propose to use a hive of VLM/LLM models to build a dataset of tile/caption pairs of high quality.\n",
    "The hope is that all these models can complement each other and make use of the available *seed* datasets.\n",
    "\n",
    "The basic idea is to mine a lot of high-quality tiles, at similar magnification, directly from the WSIs, and caption them using VLLM.\n",
    "Let's see how they do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Regarding tile mining, this need a small digression: in addition to being able to do zero-shot classification, you can do dense cross-modol retrieval also!\n",
    "\n",
    "**Dense Retrieval**: Search in a bank of images for the one that would be best described by a given text prompt - Or using the same modality: that would be closer to a given probe image.\n",
    "\n",
    "> **_Question 2.7_** How would you implement that? Write that down using the previous tiles: \"H&E that contain tumorous tissue.\"\n",
    "> Try implementing that on the embeddings i made you download in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_embeddings = torch.vstack([tumor_embeddings, stroma_embeddings])\n",
    "tiles_path = [Path(p) for p in tumor_tiles + stroma_tiles]\n",
    "text_probe = tokenizer([\"An H&E image with tumor cells\"])\n",
    "\n",
    "# Answer - dense retrieval algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "he idea of PathGen is to build captioning models (VLLMs) using the available multimodal datasets, \n",
    "in order to label a high quality dataset of tiles, extracted directly from WSIs.\n",
    "\n",
    "## Step 1: Caption Enhancement with Vision LLMs\n",
    "\n",
    "The first step is to improve the current image/captions pairs datasets (like PLIP, OpenPath, etc...).\n",
    "One of their drawbacks was their 'non-exhaustivity', i.e. these captions, coming from an educational source, assumed a lot of a-priori knowledge from the reader \n",
    "(for instance, it is not often formulated that the image is a pathology image).\n",
    "This idea is therefore to complete these captions using a general domain VLLM like GPT-4o, that can describe low level features with reasonable efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Note_**: \n",
    "> Because running an LLM on a personnal laptop is often infeasible (in google Colab as well), I had to find an alternative way of doing so. \n",
    "> If you want to do so yourself, you could either:\n",
    "> - Use endpoints of LLM providers (MistralAI, OpenAI, Cohere) and a *personal* API key\n",
    "> - Use the serverless inference services proposed by company like [HuggingFace](https://huggingface.co/learn/cookbook/enterprise_hub_serverless_inference_api) or [TogetherAI](https://www.together.ai/) etc. The service is similar to the one offered by LLM providers, but extends to open-source models. You do not have to bother about anything regarding your calls, service extends as a function of your needs (constrained by your token-per-minute limit). For personal use-case I would recommend that.\n",
    "> - Use cloud providers that allow you to deploy model on a distant device. Azure, VertexAI (google), Amazon, and even HuggingFace ([InferenceProvider](https://endpoints.huggingface.co/)) you do not pay per call but per hours of activity. \n",
    "> \n",
    "> For this course, I tried the last solution as it seemed to be the only one available to *serve* an LLM to a group of people - Without investing a lot of time into it, this seems an unstable solution. The GPU I rent kept getting overloaded and the endpoint crashed with only a few requests.\n",
    "> \n",
    "> I therefore put in place an intermediate solution: [this little huggingface space](https://huggingface.co/spaces/trizard/ai4health/tree/main), serving a small python server. Under the hood, this python server just calls one of the LLM provider (MistralAI here, cocorico) using my API key, but keeping it secure (By the way, never ever share such an API key).\n",
    "> \n",
    "> I would recommend doing so for any idea/hack using LLM that you would like to serve to a small group of people, for testing or educational (or fun) purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# And here is a simple class using the request package to submit request to the HF-space server\n",
    "class SimpleChatClient:\n",
    "    def __init__(self, endpoint, model=\"mistral-small-latest\"): #Please dont change that to **large**, I am paying for the calls :'(\n",
    "        self.endpoint = endpoint\n",
    "        self.model = model\n",
    "\n",
    "    def encode_image_to_base64(self, image_path, max_size=256, quality=30):\n",
    "        \"\"\"\n",
    "        I'm here aggressively reducing the image quality here in the hope that everyone could use this endpoint without crashing it.\n",
    "        You could just encode in base64 the image, though, in other circumstances.\n",
    "\n",
    "        Base64 encoding is a good way to send information through HTTP requests: byte encoding \n",
    "        may contain special characters that would break the request, base64 encodes chunks of \n",
    "        6 bites at a time, with 64 possible values, all contained in the standard, safe ASCII set.\n",
    "\n",
    "        Of course, the image is then decoded on the other side -i.e. in MistralAI's servers.\n",
    "        \"\"\"\n",
    "        with Image.open(image_path) as img:\n",
    "            if img.mode == 'RGBA':\n",
    "                img = img.convert('RGB')\n",
    "            ratio = max_size / max(img.size)\n",
    "            if ratio < 1:\n",
    "                new_size = tuple(int(dim * ratio) for dim in img.size)\n",
    "                img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "            buffer = io.BytesIO()\n",
    "            img.save(buffer, format='JPEG', quality=quality, optimize=True)\n",
    "            encoded_string = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "        return f\"data:image/jpeg;base64,{encoded_string}\"\n",
    "\n",
    "    def chat(self, text, image_path=None, max_tokens=150):\n",
    "        # Sends an http request to the proxy server.\n",
    "        messages = []\n",
    "        content = []\n",
    "        if text:\n",
    "            content.append({\"type\": \"text\", \"text\": text})\n",
    "        if image_path:\n",
    "            base64_image = self.encode_image_to_base64(image_path)\n",
    "            content.append({\"type\": \"image_url\", \"image_url\": base64_image})\n",
    "        messages.append({\"role\": \"user\", \"content\": content})\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "        response = requests.post(\n",
    "            self.endpoint,\n",
    "            json=payload,\n",
    "            headers=headers,\n",
    "            timeout=60\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    ">   **_Question 2.8_** Here comes a bit of prompt engineering: design a prompt to improve the caption of an image.\n",
    ">   If you want help creating a good prompt, there are online tools that do just that. \n",
    ">   For instance, Anthropic proposes this service (but you would need to have an API key). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "def get_description_prompt(caption):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 2: Fine-tune a **Revise VLLM**\n",
    "\n",
    "With this method of caption augmentation - the authors have build a dataset for fine-tuning a VLLM. \n",
    "The next step has therefore been to fine-tune a LLaVa model on it, creating a VLLM able to caption pathology tiles.\n",
    "\n",
    "We can therefore suppose in the rest of the tutorial that the Llava model we are using has extended capabilities in pathology. \n",
    "We will still use the general LLaVa-11B-Instruct because PathGen-LLaVa-desp would be too big to run locally on your machines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    " ### Step 3: Caption Revision Model\n",
    "\n",
    "PathGen-LlaVa-desp can describe pathology images better than general domain models. It is not however perfect. \n",
    "Improving the model could be done by improving the dataset - hard to do without additional expertise.\n",
    "\n",
    "The authors have bet on the training of a model able to **revise** and **correct** the captions made by PathGen-LlaVa-desp.\n",
    "\n",
    ">  **_Question 2.9_** How would you do that?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 4: Create a **revise** dataset and Fine-Tune a **Revise VLLM**\n",
    "\n",
    "To do that, we need a dataset composed of images paired with 2 captions: 1 correct and 1 incorrect.\n",
    "\n",
    "The idea here will be to degrade the LLaVa descriptions with known modifications: additions, deletions, modifications. This is a type of self-supervised learning led by the LLM (just like conventional contrastive learning in image processing perturbs a given image).\n",
    "\n",
    "We then \"just\" have to fine-tune an LLM on this dataset to get a \"Revise LLM\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's suppose that this LLama-3.2-11B has been trained to describe histopathological images.\n",
    "image_path = Path('assets') / captions[0]['filename']\n",
    "client = SimpleChatClient(\"https://trizard-ai4health.hf.space/chat\")\n",
    "completion = client.chat(\n",
    "    \"You are an expert in anatomopathology in a hospital. You are given an H&E image of a tissue sample: describe it in detail, focusing on the histological features and any potential pathological findings. You will answer only with your description of the image.\",\n",
    "    image_path=image_path\n",
    ")\n",
    "image_description = completion\n",
    "\n",
    "print(image_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Question 2.10_** Implement the pipeline to create the revise dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(Panel(\n",
    "    image_description,\n",
    "    title=\"Original Description\",\n",
    "    border_style=\"green\"\n",
    "))\n",
    "\n",
    "console.print(Panel(\n",
    "    degraded_description,\n",
    "    title=f\"Description degraded with [red]{deg_type}[/red]\",\n",
    "    border_style=\"green\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "### Step 5: Leveraging the TCGA Dataset\n",
    "\n",
    "The TCGA dataset is a public resource of Whole Slide Images. \n",
    "It contains not only WSI, but also (and mostly) other modalities, such as genomic, transcriptomic, and even text. \n",
    "Authors gathered 7300 WSI paired with their pathology reports - below is an example for the slide used in the previous notebook.\n",
    "\n",
    "We therefore now possess a **description VLLM**, a **Revise VLLM** and we also suppose that we dispose of a **Summarize LLM**, trained to summarized too long descriptions.\n",
    "\n",
    "#### Dataset Construction Pipeline\n",
    "\n",
    "We will now design a pipeline to create a high quality dataset of image/caption pairs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "> **_Question 2.11_**\n",
    "> Suppposing you have all these new tools, propose a pipeline to create a high quality dataset of image/caption pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathology_report = \"\"\"\n",
    "Sections show an infiltrating mammary carcinoma characterized by poor tubule formation, intermediate nuclear grade and high mitotic activity. \n",
    "The tumor cells infiltrate as sheets, single file, alveolar nests and occasional larger nests.\n",
    "The tumor has a tendency to infiltrate around existing ductal structures and focally formd targetoid lesions around this. \n",
    "Focal early necrosis is noted. \n",
    "There is a desmoplastic stromal response.\"\"\"\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = SimpleChatClient(\"https://trizard-ai4health.hf.space/chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Tile Sampling Strategy\n",
    "\n",
    "The idea will simply be to dig into the WSIs we have, extract the most relevant and diverse tiles, \n",
    "and then make them pass through the different VLM we have, for description, revision and summarization.\n",
    "\n",
    "#### Prompt-based Extraction: first using pathology report!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = np.load('assets/embeddings_tiles.npy')\n",
    "\n",
    "prompts_from_report = pathology_report.split('.')[:-1]\n",
    "tokenized_prompts = tokenizer(prompts_from_report)\n",
    "prompts_report_embeddings = clip_model.encode_text(tokenized_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt-based extraction: using general model knowledge\n",
    "You can also try to distill a bit of general knowledge from an LLM into this tile selection phase. \n",
    "> **_Question 2.12_** Create prompts to use for dense retrieval using a CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GPT generated prompts:\n",
    "prompts_from_gpt = [client.chat(\"You are an expert in histopathology. You are task to describe one pattern that could be present in a breast cancer slide. Answer exclusively by giving an example pattern.\", max_tokens=10) for _ in range(3)]\n",
    "tokenized_prompts = tokenizer(prompts_from_gpt)\n",
    "prompts_gpt_embeddings = clip_model.encode_text(tokenized_prompts)\n",
    "\n",
    "prompts = torch.vstack([prompts_report_embeddings, prompts_gpt_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity = cosine(torch.tensor(tiles), torch.tensor(prompts))\n",
    "cosine_similarity = torch.max(cosine_similarity, dim=1).values\n",
    "first_128_indices = torch.argsort(cosine_similarity, descending=True)[:128]\n",
    "prompt_extracted_tiles = tiles[first_128_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 2. Diversity Considerations\n",
    "\n",
    "> **_Question 2.13_** What could be one caveat of sampling tiles using only prompt-based retrieval? \n",
    "> Implement an alternative approach for tile-sampling.\n",
    "\n",
    "The goal of this dataset is to screen tiles as diverse as possible (and, if possible, containing as much interesting features as possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Finally: Caption Generation\n",
    "\n",
    "Step 2 in the process is to use the description/revise/summarize VLMs that have been trained before in order to generate new captions for this whole dataset.\n",
    "Doing so, they gather >1.6M of high-quality tile/caption pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, they finally trained a CLIP model on this new dataset, **PathGen-CLIP-L** ! The resulting model shows unprecedented accuracies in many settings.\n",
    "Let's try its capabilities with the tasks we tried to tackle before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, _, preprocess_clip = open_clip.create_model_and_transforms('ViT-L-14', pretrained='assets/pathgen-clip-l.pt')\n",
    "clip_model.eval()\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('ViT-L-14')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Question 2.14:_** Try again all the tasks we did before using the general CLIP! You can also try the dense retrieval on the WSI of the tuto 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see, this seems already way better !\n",
    "\n",
    "**Overall, this paper illustrate the perspectives that offer these weak forms of model distillation. General LLM/VLM often allow to greatly extend seed datasets, using very few guidance and supervision -Here, for instance, human input was to focus the LLM on undescribed details of the image + was the choice of the caption degradation types.**\n",
    "\n",
    "> **_Question 2.15_** However, this strategy has limitations.\n",
    "> - Author call this pipeline \"agentic\". What do you think of this naming ? Do you find it appropriate here ? \n",
    "> - Why would they restrain themselves to a mere 1.6M tile/caption pairs, when they could create tens of millions ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
